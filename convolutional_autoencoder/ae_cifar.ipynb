{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar10_batch(cifar10_dataset_folder_path, batch_id):\n",
    "    with open(cifar10_dataset_folder_path + '/data_batch_' + str(batch_id), mode='rb') as file:\n",
    "        # note the encoding type is 'latin1'\n",
    "        batch = pickle.load(file, encoding='latin1')\n",
    "        \n",
    "    features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    labels = batch['labels']\n",
    "        \n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.empty((0,32,32,3))\n",
    "labels = []\n",
    "for i in range(1, 6):\n",
    "    ft, lb = load_cifar10_batch('cifar-10', i)\n",
    "    features = np.concatenate((features, ft))\n",
    "    labels += lb\n",
    "x_train = features / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft, lb = load_cifar10_batch('cifar-10', 6)\n",
    "x_test = ft / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print (x_train.shape)\n",
    "print (len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x115647588>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11175e048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = random.randint(1, len(labels))\n",
    "print (\"Label: {}\".format(labels[idx]))\n",
    "img = x_train[idx]\n",
    "plt.imshow(img.reshape((32, 32, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 64)\n",
      "(?, 16, 16, 64)\n",
      "(?, 16, 16, 128)\n",
      "(?, 8, 8, 128)\n",
      "(?, 8, 8, 256)\n",
      "(?, 4, 4, 256)\n",
      "(?, 8, 8, 256)\n",
      "(?, 8, 8, 128)\n",
      "(?, 16, 16, 128)\n",
      "(?, 16, 16, 64)\n",
      "(?, 32, 32, 64)\n",
      "(?, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "# Input and target placeholders\n",
    "\n",
    "inputs_ = tf.placeholder(tf.float32, (None, 32, 32, 3), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 32, 32, 3), name='targets')\n",
    "\n",
    "### Encoder\n",
    "conv1 = tf.layers.conv2d(inputs_, 64, (3,3), padding='same', activation=tf.nn.relu)\n",
    "print(conv1.shape)\n",
    "# Now 32x32x64\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "print(maxpool1.shape)\n",
    "# Now 16x16x64\n",
    "conv2 = tf.layers.conv2d(maxpool1, 128, (3,3), padding='same', activation=tf.nn.relu)\n",
    "print(conv2.shape)\n",
    "# Now 16x16x128\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "print(maxpool2.shape)\n",
    "# Now 8x8x128\n",
    "conv3 = tf.layers.conv2d(maxpool2, 256, (3,3), padding='same', activation=tf.nn.relu)\n",
    "print(conv3.shape)\n",
    "# Now 8x8x256\n",
    "encoded = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
    "print(encoded.shape)\n",
    "# Now 4x4x256\n",
    "\n",
    "### Decoder\n",
    "upsample1 = tf.image.resize_nearest_neighbor(encoded, (8,8))\n",
    "print(upsample1.shape)\n",
    "# Now 8x8x256\n",
    "conv4 = tf.layers.conv2d(upsample1, 128, (3,3), padding='same', activation=tf.nn.relu)\n",
    "print(conv4.shape)\n",
    "# Now 8x8x128\n",
    "upsample2 = tf.image.resize_nearest_neighbor(conv4, (16,16))\n",
    "print(upsample2.shape)\n",
    "# Now 16x16x128\n",
    "conv5 = tf.layers.conv2d(upsample2, 64, (3,3), padding='same', activation=tf.nn.relu)\n",
    "print(conv5.shape)\n",
    "# Now 16x16x64\n",
    "upsample3 = tf.image.resize_nearest_neighbor(conv5, (32,32))\n",
    "print(upsample3.shape)\n",
    "# Now 32x32x64\n",
    "logits = tf.layers.conv2d(upsample3, 3, (3,3), padding='same', activation=None)\n",
    "print(logits.shape)\n",
    "# Now 32x32x3\n",
    "\n",
    "#logits = tf.layers.conv2d(conv6, 1, (3,3), padding='same', activation=None)\n",
    "#print(logits.shape)\n",
    "#Now 28x28x1\n",
    "\n",
    "# Pass logits through sigmoid to get reconstructed image\n",
    "decoded = tf.nn.sigmoid(logits, name='output')\n",
    "\n",
    "# Pass logits through sigmoid and calculate the cross-entropy loss\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "\n",
    "# Get cost and define the optimizer\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(ii, batch_size):\n",
    "    begin = ii * batch_size\n",
    "    end = (ii+1) * batch_size\n",
    "    return x_train[begin:end], labels[begin:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 Step: 0/390 ... Training loss: 0.6930\n",
      "Epoch: 1/20 Step: 1/390 ... Training loss: 0.6926\n",
      "Epoch: 1/20 Step: 2/390 ... Training loss: 0.6938\n",
      "Epoch: 1/20 Step: 3/390 ... Training loss: 0.6878\n",
      "Epoch: 1/20 Step: 4/390 ... Training loss: 0.6859\n",
      "Epoch: 1/20 Step: 5/390 ... Training loss: 0.6791\n",
      "Epoch: 1/20 Step: 6/390 ... Training loss: 0.6657\n",
      "Epoch: 1/20 Step: 7/390 ... Training loss: 0.6648\n",
      "Epoch: 1/20 Step: 8/390 ... Training loss: 0.7043\n",
      "Epoch: 1/20 Step: 9/390 ... Training loss: 0.6728\n",
      "Epoch: 1/20 Step: 10/390 ... Training loss: 0.6662\n",
      "Epoch: 1/20 Step: 11/390 ... Training loss: 0.6506\n",
      "Epoch: 1/20 Step: 12/390 ... Training loss: 0.6569\n",
      "Epoch: 1/20 Step: 13/390 ... Training loss: 0.6637\n",
      "Epoch: 1/20 Step: 14/390 ... Training loss: 0.6590\n",
      "Epoch: 1/20 Step: 15/390 ... Training loss: 0.6524\n",
      "Epoch: 1/20 Step: 16/390 ... Training loss: 0.6585\n",
      "Epoch: 1/20 Step: 17/390 ... Training loss: 0.6442\n",
      "Epoch: 1/20 Step: 18/390 ... Training loss: 0.6312\n",
      "Epoch: 1/20 Step: 19/390 ... Training loss: 0.6333\n",
      "Epoch: 1/20 Step: 20/390 ... Training loss: 0.6237\n",
      "Epoch: 1/20 Step: 21/390 ... Training loss: 0.6301\n",
      "Epoch: 1/20 Step: 22/390 ... Training loss: 0.6318\n",
      "Epoch: 1/20 Step: 23/390 ... Training loss: 0.6209\n",
      "Epoch: 1/20 Step: 24/390 ... Training loss: 0.6294\n",
      "Epoch: 1/20 Step: 25/390 ... Training loss: 0.6140\n",
      "Epoch: 1/20 Step: 26/390 ... Training loss: 0.6215\n",
      "Epoch: 1/20 Step: 27/390 ... Training loss: 0.6150\n",
      "Epoch: 1/20 Step: 28/390 ... Training loss: 0.6135\n",
      "Epoch: 1/20 Step: 29/390 ... Training loss: 0.6053\n",
      "Epoch: 1/20 Step: 30/390 ... Training loss: 0.6188\n",
      "Epoch: 1/20 Step: 31/390 ... Training loss: 0.6055\n",
      "Epoch: 1/20 Step: 32/390 ... Training loss: 0.6133\n",
      "Epoch: 1/20 Step: 33/390 ... Training loss: 0.6074\n",
      "Epoch: 1/20 Step: 34/390 ... Training loss: 0.6034\n",
      "Epoch: 1/20 Step: 35/390 ... Training loss: 0.5952\n",
      "Epoch: 1/20 Step: 36/390 ... Training loss: 0.6001\n",
      "Epoch: 1/20 Step: 37/390 ... Training loss: 0.6026\n",
      "Epoch: 1/20 Step: 38/390 ... Training loss: 0.5980\n",
      "Epoch: 1/20 Step: 39/390 ... Training loss: 0.5997\n",
      "Epoch: 1/20 Step: 40/390 ... Training loss: 0.5882\n",
      "Epoch: 1/20 Step: 41/390 ... Training loss: 0.6021\n",
      "Epoch: 1/20 Step: 42/390 ... Training loss: 0.5943\n",
      "Epoch: 1/20 Step: 43/390 ... Training loss: 0.6005\n",
      "Epoch: 1/20 Step: 44/390 ... Training loss: 0.5971\n",
      "Epoch: 1/20 Step: 45/390 ... Training loss: 0.5866\n",
      "Epoch: 1/20 Step: 46/390 ... Training loss: 0.6015\n",
      "Epoch: 1/20 Step: 47/390 ... Training loss: 0.5899\n",
      "Epoch: 1/20 Step: 48/390 ... Training loss: 0.5956\n",
      "Epoch: 1/20 Step: 49/390 ... Training loss: 0.5941\n",
      "Epoch: 1/20 Step: 50/390 ... Training loss: 0.5897\n",
      "Epoch: 1/20 Step: 51/390 ... Training loss: 0.5961\n",
      "Epoch: 1/20 Step: 52/390 ... Training loss: 0.5955\n",
      "Epoch: 1/20 Step: 53/390 ... Training loss: 0.6024\n",
      "Epoch: 1/20 Step: 54/390 ... Training loss: 0.5992\n",
      "Epoch: 1/20 Step: 55/390 ... Training loss: 0.5850\n",
      "Epoch: 1/20 Step: 56/390 ... Training loss: 0.5855\n",
      "Epoch: 1/20 Step: 57/390 ... Training loss: 0.5983\n",
      "Epoch: 1/20 Step: 58/390 ... Training loss: 0.5809\n",
      "Epoch: 1/20 Step: 59/390 ... Training loss: 0.5942\n",
      "Epoch: 1/20 Step: 60/390 ... Training loss: 0.5818\n",
      "Epoch: 1/20 Step: 61/390 ... Training loss: 0.5875\n",
      "Epoch: 1/20 Step: 62/390 ... Training loss: 0.5823\n",
      "Epoch: 1/20 Step: 63/390 ... Training loss: 0.5898\n",
      "Epoch: 1/20 Step: 64/390 ... Training loss: 0.5853\n",
      "Epoch: 1/20 Step: 65/390 ... Training loss: 0.5825\n",
      "Epoch: 1/20 Step: 66/390 ... Training loss: 0.5910\n",
      "Epoch: 1/20 Step: 67/390 ... Training loss: 0.5833\n",
      "Epoch: 1/20 Step: 68/390 ... Training loss: 0.5805\n",
      "Epoch: 1/20 Step: 69/390 ... Training loss: 0.5846\n",
      "Epoch: 1/20 Step: 70/390 ... Training loss: 0.5874\n",
      "Epoch: 1/20 Step: 71/390 ... Training loss: 0.5940\n",
      "Epoch: 1/20 Step: 72/390 ... Training loss: 0.5836\n",
      "Epoch: 1/20 Step: 73/390 ... Training loss: 0.5890\n",
      "Epoch: 1/20 Step: 74/390 ... Training loss: 0.5855\n",
      "Epoch: 1/20 Step: 75/390 ... Training loss: 0.5973\n",
      "Epoch: 1/20 Step: 76/390 ... Training loss: 0.5883\n",
      "Epoch: 1/20 Step: 77/390 ... Training loss: 0.5888\n",
      "Epoch: 1/20 Step: 78/390 ... Training loss: 0.5869\n",
      "Epoch: 1/20 Step: 79/390 ... Training loss: 0.5767\n",
      "Epoch: 1/20 Step: 80/390 ... Training loss: 0.5864\n",
      "Epoch: 1/20 Step: 81/390 ... Training loss: 0.5868\n",
      "Epoch: 1/20 Step: 82/390 ... Training loss: 0.5818\n",
      "Epoch: 1/20 Step: 83/390 ... Training loss: 0.5859\n",
      "Epoch: 1/20 Step: 84/390 ... Training loss: 0.5840\n",
      "Epoch: 1/20 Step: 85/390 ... Training loss: 0.5853\n",
      "Epoch: 1/20 Step: 86/390 ... Training loss: 0.5824\n",
      "Epoch: 1/20 Step: 87/390 ... Training loss: 0.5889\n",
      "Epoch: 1/20 Step: 88/390 ... Training loss: 0.5945\n",
      "Epoch: 1/20 Step: 89/390 ... Training loss: 0.5940\n",
      "Epoch: 1/20 Step: 90/390 ... Training loss: 0.5901\n",
      "Epoch: 1/20 Step: 91/390 ... Training loss: 0.5926\n",
      "Epoch: 1/20 Step: 92/390 ... Training loss: 0.5821\n",
      "Epoch: 1/20 Step: 93/390 ... Training loss: 0.5968\n",
      "Epoch: 1/20 Step: 94/390 ... Training loss: 0.5862\n",
      "Epoch: 1/20 Step: 95/390 ... Training loss: 0.5909\n",
      "Epoch: 1/20 Step: 96/390 ... Training loss: 0.5861\n",
      "Epoch: 1/20 Step: 97/390 ... Training loss: 0.5856\n",
      "Epoch: 1/20 Step: 98/390 ... Training loss: 0.5836\n",
      "Epoch: 1/20 Step: 99/390 ... Training loss: 0.5987\n",
      "Epoch: 1/20 Step: 100/390 ... Training loss: 0.5723\n",
      "Epoch: 1/20 Step: 101/390 ... Training loss: 0.5840\n",
      "Epoch: 1/20 Step: 102/390 ... Training loss: 0.5762\n",
      "Epoch: 1/20 Step: 103/390 ... Training loss: 0.5762\n",
      "Epoch: 1/20 Step: 104/390 ... Training loss: 0.5899\n",
      "Epoch: 1/20 Step: 105/390 ... Training loss: 0.5792\n",
      "Epoch: 1/20 Step: 106/390 ... Training loss: 0.5915\n",
      "Epoch: 1/20 Step: 107/390 ... Training loss: 0.5785\n",
      "Epoch: 1/20 Step: 108/390 ... Training loss: 0.5828\n",
      "Epoch: 1/20 Step: 109/390 ... Training loss: 0.5791\n",
      "Epoch: 1/20 Step: 110/390 ... Training loss: 0.5827\n",
      "Epoch: 1/20 Step: 111/390 ... Training loss: 0.5815\n",
      "Epoch: 1/20 Step: 112/390 ... Training loss: 0.5766\n",
      "Epoch: 1/20 Step: 113/390 ... Training loss: 0.5758\n",
      "Epoch: 1/20 Step: 114/390 ... Training loss: 0.5833\n",
      "Epoch: 1/20 Step: 115/390 ... Training loss: 0.5811\n",
      "Epoch: 1/20 Step: 116/390 ... Training loss: 0.5902\n",
      "Epoch: 1/20 Step: 117/390 ... Training loss: 0.5694\n",
      "Epoch: 1/20 Step: 118/390 ... Training loss: 0.5814\n",
      "Epoch: 1/20 Step: 119/390 ... Training loss: 0.5775\n",
      "Epoch: 1/20 Step: 120/390 ... Training loss: 0.5754\n",
      "Epoch: 1/20 Step: 121/390 ... Training loss: 0.5821\n",
      "Epoch: 1/20 Step: 122/390 ... Training loss: 0.5797\n",
      "Epoch: 1/20 Step: 123/390 ... Training loss: 0.5809\n",
      "Epoch: 1/20 Step: 124/390 ... Training loss: 0.5748\n",
      "Epoch: 1/20 Step: 125/390 ... Training loss: 0.5816\n",
      "Epoch: 1/20 Step: 126/390 ... Training loss: 0.5714\n",
      "Epoch: 1/20 Step: 127/390 ... Training loss: 0.5706\n",
      "Epoch: 1/20 Step: 128/390 ... Training loss: 0.5787\n",
      "Epoch: 1/20 Step: 129/390 ... Training loss: 0.5754\n",
      "Epoch: 1/20 Step: 130/390 ... Training loss: 0.5807\n",
      "Epoch: 1/20 Step: 131/390 ... Training loss: 0.5863\n",
      "Epoch: 1/20 Step: 132/390 ... Training loss: 0.5751\n",
      "Epoch: 1/20 Step: 133/390 ... Training loss: 0.5781\n",
      "Epoch: 1/20 Step: 134/390 ... Training loss: 0.5735\n",
      "Epoch: 1/20 Step: 135/390 ... Training loss: 0.5811\n",
      "Epoch: 1/20 Step: 136/390 ... Training loss: 0.5861\n",
      "Epoch: 1/20 Step: 137/390 ... Training loss: 0.5951\n",
      "Epoch: 1/20 Step: 138/390 ... Training loss: 0.5892\n",
      "Epoch: 1/20 Step: 139/390 ... Training loss: 0.5838\n",
      "Epoch: 1/20 Step: 140/390 ... Training loss: 0.5814\n",
      "Epoch: 1/20 Step: 141/390 ... Training loss: 0.5857\n",
      "Epoch: 1/20 Step: 142/390 ... Training loss: 0.5789\n",
      "Epoch: 1/20 Step: 143/390 ... Training loss: 0.5799\n",
      "Epoch: 1/20 Step: 144/390 ... Training loss: 0.5874\n",
      "Epoch: 1/20 Step: 145/390 ... Training loss: 0.5750\n",
      "Epoch: 1/20 Step: 146/390 ... Training loss: 0.5723\n",
      "Epoch: 1/20 Step: 147/390 ... Training loss: 0.5754\n",
      "Epoch: 1/20 Step: 148/390 ... Training loss: 0.5856\n",
      "Epoch: 1/20 Step: 149/390 ... Training loss: 0.5665\n",
      "Epoch: 1/20 Step: 150/390 ... Training loss: 0.5709\n",
      "Epoch: 1/20 Step: 151/390 ... Training loss: 0.5806\n",
      "Epoch: 1/20 Step: 152/390 ... Training loss: 0.5779\n",
      "Epoch: 1/20 Step: 153/390 ... Training loss: 0.5679\n",
      "Epoch: 1/20 Step: 154/390 ... Training loss: 0.5779\n",
      "Epoch: 1/20 Step: 155/390 ... Training loss: 0.5793\n",
      "Epoch: 1/20 Step: 156/390 ... Training loss: 0.5785\n",
      "Epoch: 1/20 Step: 157/390 ... Training loss: 0.5693\n",
      "Epoch: 1/20 Step: 158/390 ... Training loss: 0.5726\n",
      "Epoch: 1/20 Step: 159/390 ... Training loss: 0.5830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 Step: 160/390 ... Training loss: 0.5753\n",
      "Epoch: 1/20 Step: 161/390 ... Training loss: 0.5758\n",
      "Epoch: 1/20 Step: 162/390 ... Training loss: 0.5790\n",
      "Epoch: 1/20 Step: 163/390 ... Training loss: 0.5656\n",
      "Epoch: 1/20 Step: 164/390 ... Training loss: 0.5654\n",
      "Epoch: 1/20 Step: 165/390 ... Training loss: 0.5813\n",
      "Epoch: 1/20 Step: 166/390 ... Training loss: 0.5835\n",
      "Epoch: 1/20 Step: 167/390 ... Training loss: 0.5753\n",
      "Epoch: 1/20 Step: 168/390 ... Training loss: 0.5733\n",
      "Epoch: 1/20 Step: 169/390 ... Training loss: 0.5711\n",
      "Epoch: 1/20 Step: 170/390 ... Training loss: 0.5720\n",
      "Epoch: 1/20 Step: 171/390 ... Training loss: 0.5664\n",
      "Epoch: 1/20 Step: 172/390 ... Training loss: 0.5778\n",
      "Epoch: 1/20 Step: 173/390 ... Training loss: 0.5728\n",
      "Epoch: 1/20 Step: 174/390 ... Training loss: 0.5917\n",
      "Epoch: 1/20 Step: 175/390 ... Training loss: 0.5692\n",
      "Epoch: 1/20 Step: 176/390 ... Training loss: 0.5743\n",
      "Epoch: 1/20 Step: 177/390 ... Training loss: 0.5776\n",
      "Epoch: 1/20 Step: 178/390 ... Training loss: 0.5756\n",
      "Epoch: 1/20 Step: 179/390 ... Training loss: 0.5734\n",
      "Epoch: 1/20 Step: 180/390 ... Training loss: 0.5761\n",
      "Epoch: 1/20 Step: 181/390 ... Training loss: 0.5878\n",
      "Epoch: 1/20 Step: 182/390 ... Training loss: 0.5743\n",
      "Epoch: 1/20 Step: 183/390 ... Training loss: 0.5772\n",
      "Epoch: 1/20 Step: 184/390 ... Training loss: 0.5683\n",
      "Epoch: 1/20 Step: 185/390 ... Training loss: 0.5619\n",
      "Epoch: 1/20 Step: 186/390 ... Training loss: 0.5735\n",
      "Epoch: 1/20 Step: 187/390 ... Training loss: 0.5780\n",
      "Epoch: 1/20 Step: 188/390 ... Training loss: 0.5733\n",
      "Epoch: 1/20 Step: 189/390 ... Training loss: 0.5760\n",
      "Epoch: 1/20 Step: 190/390 ... Training loss: 0.5826\n",
      "Epoch: 1/20 Step: 191/390 ... Training loss: 0.5730\n",
      "Epoch: 1/20 Step: 192/390 ... Training loss: 0.5851\n",
      "Epoch: 1/20 Step: 193/390 ... Training loss: 0.5582\n",
      "Epoch: 1/20 Step: 194/390 ... Training loss: 0.5844\n",
      "Epoch: 1/20 Step: 195/390 ... Training loss: 0.5741\n",
      "Epoch: 1/20 Step: 196/390 ... Training loss: 0.5735\n",
      "Epoch: 1/20 Step: 197/390 ... Training loss: 0.5806\n",
      "Epoch: 1/20 Step: 198/390 ... Training loss: 0.5733\n",
      "Epoch: 1/20 Step: 199/390 ... Training loss: 0.5799\n",
      "Epoch: 1/20 Step: 200/390 ... Training loss: 0.5711\n",
      "Epoch: 1/20 Step: 201/390 ... Training loss: 0.5788\n",
      "Epoch: 1/20 Step: 202/390 ... Training loss: 0.5817\n",
      "Epoch: 1/20 Step: 203/390 ... Training loss: 0.5639\n",
      "Epoch: 1/20 Step: 204/390 ... Training loss: 0.5652\n",
      "Epoch: 1/20 Step: 205/390 ... Training loss: 0.5703\n",
      "Epoch: 1/20 Step: 206/390 ... Training loss: 0.5873\n",
      "Epoch: 1/20 Step: 207/390 ... Training loss: 0.5714\n",
      "Epoch: 1/20 Step: 208/390 ... Training loss: 0.5761\n",
      "Epoch: 1/20 Step: 209/390 ... Training loss: 0.5819\n",
      "Epoch: 1/20 Step: 210/390 ... Training loss: 0.5832\n",
      "Epoch: 1/20 Step: 211/390 ... Training loss: 0.5781\n",
      "Epoch: 1/20 Step: 212/390 ... Training loss: 0.5753\n",
      "Epoch: 1/20 Step: 213/390 ... Training loss: 0.5698\n",
      "Epoch: 1/20 Step: 214/390 ... Training loss: 0.5781\n",
      "Epoch: 1/20 Step: 215/390 ... Training loss: 0.5705\n",
      "Epoch: 1/20 Step: 216/390 ... Training loss: 0.5628\n",
      "Epoch: 1/20 Step: 217/390 ... Training loss: 0.5673\n",
      "Epoch: 1/20 Step: 218/390 ... Training loss: 0.5744\n",
      "Epoch: 1/20 Step: 219/390 ... Training loss: 0.5667\n",
      "Epoch: 1/20 Step: 220/390 ... Training loss: 0.5840\n",
      "Epoch: 1/20 Step: 221/390 ... Training loss: 0.5778\n",
      "Epoch: 1/20 Step: 222/390 ... Training loss: 0.5715\n",
      "Epoch: 1/20 Step: 223/390 ... Training loss: 0.5738\n",
      "Epoch: 1/20 Step: 224/390 ... Training loss: 0.5731\n",
      "Epoch: 1/20 Step: 225/390 ... Training loss: 0.5881\n",
      "Epoch: 1/20 Step: 226/390 ... Training loss: 0.5803\n",
      "Epoch: 1/20 Step: 227/390 ... Training loss: 0.5627\n",
      "Epoch: 1/20 Step: 228/390 ... Training loss: 0.5580\n",
      "Epoch: 1/20 Step: 229/390 ... Training loss: 0.5891\n",
      "Epoch: 1/20 Step: 230/390 ... Training loss: 0.5699\n",
      "Epoch: 1/20 Step: 231/390 ... Training loss: 0.5672\n",
      "Epoch: 1/20 Step: 232/390 ... Training loss: 0.5719\n",
      "Epoch: 1/20 Step: 233/390 ... Training loss: 0.5786\n",
      "Epoch: 1/20 Step: 234/390 ... Training loss: 0.5672\n",
      "Epoch: 1/20 Step: 235/390 ... Training loss: 0.5724\n",
      "Epoch: 1/20 Step: 236/390 ... Training loss: 0.5739\n",
      "Epoch: 1/20 Step: 237/390 ... Training loss: 0.5787\n",
      "Epoch: 1/20 Step: 238/390 ... Training loss: 0.5724\n",
      "Epoch: 1/20 Step: 239/390 ... Training loss: 0.5651\n",
      "Epoch: 1/20 Step: 240/390 ... Training loss: 0.5765\n",
      "Epoch: 1/20 Step: 241/390 ... Training loss: 0.5788\n",
      "Epoch: 1/20 Step: 242/390 ... Training loss: 0.5750\n",
      "Epoch: 1/20 Step: 243/390 ... Training loss: 0.5771\n",
      "Epoch: 1/20 Step: 244/390 ... Training loss: 0.5650\n",
      "Epoch: 1/20 Step: 245/390 ... Training loss: 0.5830\n",
      "Epoch: 1/20 Step: 246/390 ... Training loss: 0.5716\n",
      "Epoch: 1/20 Step: 247/390 ... Training loss: 0.5700\n",
      "Epoch: 1/20 Step: 248/390 ... Training loss: 0.5752\n",
      "Epoch: 1/20 Step: 249/390 ... Training loss: 0.5694\n",
      "Epoch: 1/20 Step: 250/390 ... Training loss: 0.5715\n",
      "Epoch: 1/20 Step: 251/390 ... Training loss: 0.5639\n",
      "Epoch: 1/20 Step: 252/390 ... Training loss: 0.5672\n",
      "Epoch: 1/20 Step: 253/390 ... Training loss: 0.5663\n",
      "Epoch: 1/20 Step: 254/390 ... Training loss: 0.5921\n",
      "Epoch: 1/20 Step: 255/390 ... Training loss: 0.5883\n",
      "Epoch: 1/20 Step: 256/390 ... Training loss: 0.5770\n",
      "Epoch: 1/20 Step: 257/390 ... Training loss: 0.5689\n",
      "Epoch: 1/20 Step: 258/390 ... Training loss: 0.5618\n",
      "Epoch: 1/20 Step: 259/390 ... Training loss: 0.5726\n",
      "Epoch: 1/20 Step: 260/390 ... Training loss: 0.5832\n",
      "Epoch: 1/20 Step: 261/390 ... Training loss: 0.5861\n",
      "Epoch: 1/20 Step: 262/390 ... Training loss: 0.5742\n",
      "Epoch: 1/20 Step: 263/390 ... Training loss: 0.5756\n",
      "Epoch: 1/20 Step: 264/390 ... Training loss: 0.5643\n",
      "Epoch: 1/20 Step: 265/390 ... Training loss: 0.5695\n",
      "Epoch: 1/20 Step: 266/390 ... Training loss: 0.5761\n",
      "Epoch: 1/20 Step: 267/390 ... Training loss: 0.5656\n",
      "Epoch: 1/20 Step: 268/390 ... Training loss: 0.5739\n",
      "Epoch: 1/20 Step: 269/390 ... Training loss: 0.5603\n",
      "Epoch: 1/20 Step: 270/390 ... Training loss: 0.5746\n",
      "Epoch: 1/20 Step: 271/390 ... Training loss: 0.5766\n",
      "Epoch: 1/20 Step: 272/390 ... Training loss: 0.5683\n",
      "Epoch: 1/20 Step: 273/390 ... Training loss: 0.5719\n",
      "Epoch: 1/20 Step: 274/390 ... Training loss: 0.5765\n",
      "Epoch: 1/20 Step: 275/390 ... Training loss: 0.5631\n",
      "Epoch: 1/20 Step: 276/390 ... Training loss: 0.5685\n",
      "Epoch: 1/20 Step: 277/390 ... Training loss: 0.5729\n",
      "Epoch: 1/20 Step: 278/390 ... Training loss: 0.5720\n",
      "Epoch: 1/20 Step: 279/390 ... Training loss: 0.5818\n",
      "Epoch: 1/20 Step: 280/390 ... Training loss: 0.5844\n",
      "Epoch: 1/20 Step: 281/390 ... Training loss: 0.5646\n",
      "Epoch: 1/20 Step: 282/390 ... Training loss: 0.5780\n",
      "Epoch: 1/20 Step: 283/390 ... Training loss: 0.5757\n",
      "Epoch: 1/20 Step: 284/390 ... Training loss: 0.5788\n",
      "Epoch: 1/20 Step: 285/390 ... Training loss: 0.5908\n",
      "Epoch: 1/20 Step: 286/390 ... Training loss: 0.5747\n",
      "Epoch: 1/20 Step: 287/390 ... Training loss: 0.5880\n",
      "Epoch: 1/20 Step: 288/390 ... Training loss: 0.5838\n",
      "Epoch: 1/20 Step: 289/390 ... Training loss: 0.5744\n",
      "Epoch: 1/20 Step: 290/390 ... Training loss: 0.5734\n",
      "Epoch: 1/20 Step: 291/390 ... Training loss: 0.5794\n",
      "Epoch: 1/20 Step: 292/390 ... Training loss: 0.5706\n",
      "Epoch: 1/20 Step: 293/390 ... Training loss: 0.5803\n",
      "Epoch: 1/20 Step: 294/390 ... Training loss: 0.5728\n",
      "Epoch: 1/20 Step: 295/390 ... Training loss: 0.5753\n",
      "Epoch: 1/20 Step: 296/390 ... Training loss: 0.5648\n",
      "Epoch: 1/20 Step: 297/390 ... Training loss: 0.5768\n",
      "Epoch: 1/20 Step: 298/390 ... Training loss: 0.5727\n",
      "Epoch: 1/20 Step: 299/390 ... Training loss: 0.5779\n",
      "Epoch: 1/20 Step: 300/390 ... Training loss: 0.5732\n",
      "Epoch: 1/20 Step: 301/390 ... Training loss: 0.5801\n",
      "Epoch: 1/20 Step: 302/390 ... Training loss: 0.5852\n",
      "Epoch: 1/20 Step: 303/390 ... Training loss: 0.5698\n",
      "Epoch: 1/20 Step: 304/390 ... Training loss: 0.5740\n",
      "Epoch: 1/20 Step: 305/390 ... Training loss: 0.5709\n",
      "Epoch: 1/20 Step: 306/390 ... Training loss: 0.5715\n",
      "Epoch: 1/20 Step: 307/390 ... Training loss: 0.5784\n",
      "Epoch: 1/20 Step: 308/390 ... Training loss: 0.5666\n",
      "Epoch: 1/20 Step: 309/390 ... Training loss: 0.5847\n",
      "Epoch: 1/20 Step: 310/390 ... Training loss: 0.5708\n",
      "Epoch: 1/20 Step: 311/390 ... Training loss: 0.5821\n",
      "Epoch: 1/20 Step: 312/390 ... Training loss: 0.5804\n",
      "Epoch: 1/20 Step: 313/390 ... Training loss: 0.5675\n",
      "Epoch: 1/20 Step: 314/390 ... Training loss: 0.5731\n",
      "Epoch: 1/20 Step: 315/390 ... Training loss: 0.5630\n",
      "Epoch: 1/20 Step: 316/390 ... Training loss: 0.5759\n",
      "Epoch: 1/20 Step: 317/390 ... Training loss: 0.5678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20 Step: 318/390 ... Training loss: 0.5763\n",
      "Epoch: 1/20 Step: 319/390 ... Training loss: 0.5633\n",
      "Epoch: 1/20 Step: 320/390 ... Training loss: 0.5694\n",
      "Epoch: 1/20 Step: 321/390 ... Training loss: 0.5708\n",
      "Epoch: 1/20 Step: 322/390 ... Training loss: 0.5702\n",
      "Epoch: 1/20 Step: 323/390 ... Training loss: 0.5756\n",
      "Epoch: 1/20 Step: 324/390 ... Training loss: 0.5743\n",
      "Epoch: 1/20 Step: 325/390 ... Training loss: 0.5658\n",
      "Epoch: 1/20 Step: 326/390 ... Training loss: 0.5765\n",
      "Epoch: 1/20 Step: 327/390 ... Training loss: 0.5627\n",
      "Epoch: 1/20 Step: 328/390 ... Training loss: 0.5747\n",
      "Epoch: 1/20 Step: 329/390 ... Training loss: 0.5821\n",
      "Epoch: 1/20 Step: 330/390 ... Training loss: 0.5736\n",
      "Epoch: 1/20 Step: 331/390 ... Training loss: 0.5781\n",
      "Epoch: 1/20 Step: 332/390 ... Training loss: 0.5681\n",
      "Epoch: 1/20 Step: 333/390 ... Training loss: 0.5704\n",
      "Epoch: 1/20 Step: 334/390 ... Training loss: 0.5611\n",
      "Epoch: 1/20 Step: 335/390 ... Training loss: 0.5710\n",
      "Epoch: 1/20 Step: 336/390 ... Training loss: 0.5680\n",
      "Epoch: 1/20 Step: 337/390 ... Training loss: 0.5670\n",
      "Epoch: 1/20 Step: 338/390 ... Training loss: 0.5734\n",
      "Epoch: 1/20 Step: 339/390 ... Training loss: 0.5712\n",
      "Epoch: 1/20 Step: 340/390 ... Training loss: 0.5646\n",
      "Epoch: 1/20 Step: 341/390 ... Training loss: 0.5750\n",
      "Epoch: 1/20 Step: 342/390 ... Training loss: 0.5678\n",
      "Epoch: 1/20 Step: 343/390 ... Training loss: 0.5693\n",
      "Epoch: 1/20 Step: 344/390 ... Training loss: 0.5817\n",
      "Epoch: 1/20 Step: 345/390 ... Training loss: 0.5670\n",
      "Epoch: 1/20 Step: 346/390 ... Training loss: 0.5717\n",
      "Epoch: 1/20 Step: 347/390 ... Training loss: 0.5628\n",
      "Epoch: 1/20 Step: 348/390 ... Training loss: 0.5726\n",
      "Epoch: 1/20 Step: 349/390 ... Training loss: 0.5635\n",
      "Epoch: 1/20 Step: 350/390 ... Training loss: 0.5701\n",
      "Epoch: 1/20 Step: 351/390 ... Training loss: 0.5698\n",
      "Epoch: 1/20 Step: 352/390 ... Training loss: 0.5606\n",
      "Epoch: 1/20 Step: 353/390 ... Training loss: 0.5710\n",
      "Epoch: 1/20 Step: 354/390 ... Training loss: 0.5737\n",
      "Epoch: 1/20 Step: 355/390 ... Training loss: 0.5762\n",
      "Epoch: 1/20 Step: 356/390 ... Training loss: 0.5912\n",
      "Epoch: 1/20 Step: 357/390 ... Training loss: 0.5788\n",
      "Epoch: 1/20 Step: 358/390 ... Training loss: 0.5725\n",
      "Epoch: 1/20 Step: 359/390 ... Training loss: 0.5760\n",
      "Epoch: 1/20 Step: 360/390 ... Training loss: 0.5681\n",
      "Epoch: 1/20 Step: 361/390 ... Training loss: 0.5642\n",
      "Epoch: 1/20 Step: 362/390 ... Training loss: 0.5780\n",
      "Epoch: 1/20 Step: 363/390 ... Training loss: 0.5840\n",
      "Epoch: 1/20 Step: 364/390 ... Training loss: 0.5672\n",
      "Epoch: 1/20 Step: 365/390 ... Training loss: 0.5791\n",
      "Epoch: 1/20 Step: 366/390 ... Training loss: 0.5732\n",
      "Epoch: 1/20 Step: 367/390 ... Training loss: 0.5701\n",
      "Epoch: 1/20 Step: 368/390 ... Training loss: 0.5658\n",
      "Epoch: 1/20 Step: 369/390 ... Training loss: 0.5790\n",
      "Epoch: 1/20 Step: 370/390 ... Training loss: 0.5771\n",
      "Epoch: 1/20 Step: 371/390 ... Training loss: 0.5746\n",
      "Epoch: 1/20 Step: 372/390 ... Training loss: 0.5730\n",
      "Epoch: 1/20 Step: 373/390 ... Training loss: 0.5818\n",
      "Epoch: 1/20 Step: 374/390 ... Training loss: 0.5728\n",
      "Epoch: 1/20 Step: 375/390 ... Training loss: 0.5421\n",
      "Epoch: 1/20 Step: 376/390 ... Training loss: 0.5777\n",
      "Epoch: 1/20 Step: 377/390 ... Training loss: 0.5732\n",
      "Epoch: 1/20 Step: 378/390 ... Training loss: 0.5649\n",
      "Epoch: 1/20 Step: 379/390 ... Training loss: 0.5656\n",
      "Epoch: 1/20 Step: 380/390 ... Training loss: 0.5720\n",
      "Epoch: 1/20 Step: 381/390 ... Training loss: 0.5813\n",
      "Epoch: 1/20 Step: 382/390 ... Training loss: 0.5731\n",
      "Epoch: 1/20 Step: 383/390 ... Training loss: 0.5776\n",
      "Epoch: 1/20 Step: 384/390 ... Training loss: 0.5797\n",
      "Epoch: 1/20 Step: 385/390 ... Training loss: 0.5782\n",
      "Epoch: 1/20 Step: 386/390 ... Training loss: 0.5525\n",
      "Epoch: 1/20 Step: 387/390 ... Training loss: 0.5668\n",
      "Epoch: 1/20 Step: 388/390 ... Training loss: 0.5712\n",
      "Epoch: 1/20 Step: 389/390 ... Training loss: 0.5616\n",
      "Epoch: 2/20 Step: 0/390 ... Training loss: 0.5753\n",
      "Epoch: 2/20 Step: 1/390 ... Training loss: 0.5753\n",
      "Epoch: 2/20 Step: 2/390 ... Training loss: 0.5717\n",
      "Epoch: 2/20 Step: 3/390 ... Training loss: 0.5622\n",
      "Epoch: 2/20 Step: 4/390 ... Training loss: 0.5704\n",
      "Epoch: 2/20 Step: 5/390 ... Training loss: 0.5775\n",
      "Epoch: 2/20 Step: 6/390 ... Training loss: 0.5565\n",
      "Epoch: 2/20 Step: 7/390 ... Training loss: 0.5761\n",
      "Epoch: 2/20 Step: 8/390 ... Training loss: 0.5692\n",
      "Epoch: 2/20 Step: 9/390 ... Training loss: 0.5705\n",
      "Epoch: 2/20 Step: 10/390 ... Training loss: 0.5747\n",
      "Epoch: 2/20 Step: 11/390 ... Training loss: 0.5677\n",
      "Epoch: 2/20 Step: 12/390 ... Training loss: 0.5724\n",
      "Epoch: 2/20 Step: 13/390 ... Training loss: 0.5722\n",
      "Epoch: 2/20 Step: 14/390 ... Training loss: 0.5761\n",
      "Epoch: 2/20 Step: 15/390 ... Training loss: 0.5722\n",
      "Epoch: 2/20 Step: 16/390 ... Training loss: 0.5742\n",
      "Epoch: 2/20 Step: 17/390 ... Training loss: 0.5704\n",
      "Epoch: 2/20 Step: 18/390 ... Training loss: 0.5589\n",
      "Epoch: 2/20 Step: 19/390 ... Training loss: 0.5702\n",
      "Epoch: 2/20 Step: 20/390 ... Training loss: 0.5613\n",
      "Epoch: 2/20 Step: 21/390 ... Training loss: 0.5680\n",
      "Epoch: 2/20 Step: 22/390 ... Training loss: 0.5756\n",
      "Epoch: 2/20 Step: 23/390 ... Training loss: 0.5672\n",
      "Epoch: 2/20 Step: 24/390 ... Training loss: 0.5631\n",
      "Epoch: 2/20 Step: 25/390 ... Training loss: 0.5591\n",
      "Epoch: 2/20 Step: 26/390 ... Training loss: 0.5670\n",
      "Epoch: 2/20 Step: 27/390 ... Training loss: 0.5666\n",
      "Epoch: 2/20 Step: 28/390 ... Training loss: 0.5591\n",
      "Epoch: 2/20 Step: 29/390 ... Training loss: 0.5542\n",
      "Epoch: 2/20 Step: 30/390 ... Training loss: 0.5704\n",
      "Epoch: 2/20 Step: 31/390 ... Training loss: 0.5640\n",
      "Epoch: 2/20 Step: 32/390 ... Training loss: 0.5745\n",
      "Epoch: 2/20 Step: 33/390 ... Training loss: 0.5679\n",
      "Epoch: 2/20 Step: 34/390 ... Training loss: 0.5605\n",
      "Epoch: 2/20 Step: 35/390 ... Training loss: 0.5607\n",
      "Epoch: 2/20 Step: 36/390 ... Training loss: 0.5659\n",
      "Epoch: 2/20 Step: 37/390 ... Training loss: 0.5687\n",
      "Epoch: 2/20 Step: 38/390 ... Training loss: 0.5671\n",
      "Epoch: 2/20 Step: 39/390 ... Training loss: 0.5702\n",
      "Epoch: 2/20 Step: 40/390 ... Training loss: 0.5627\n",
      "Epoch: 2/20 Step: 41/390 ... Training loss: 0.5738\n",
      "Epoch: 2/20 Step: 42/390 ... Training loss: 0.5678\n",
      "Epoch: 2/20 Step: 43/390 ... Training loss: 0.5706\n",
      "Epoch: 2/20 Step: 44/390 ... Training loss: 0.5692\n",
      "Epoch: 2/20 Step: 45/390 ... Training loss: 0.5620\n",
      "Epoch: 2/20 Step: 46/390 ... Training loss: 0.5747\n",
      "Epoch: 2/20 Step: 47/390 ... Training loss: 0.5652\n",
      "Epoch: 2/20 Step: 48/390 ... Training loss: 0.5726\n",
      "Epoch: 2/20 Step: 49/390 ... Training loss: 0.5671\n",
      "Epoch: 2/20 Step: 50/390 ... Training loss: 0.5660\n",
      "Epoch: 2/20 Step: 51/390 ... Training loss: 0.5733\n",
      "Epoch: 2/20 Step: 52/390 ... Training loss: 0.5690\n",
      "Epoch: 2/20 Step: 53/390 ... Training loss: 0.5814\n",
      "Epoch: 2/20 Step: 54/390 ... Training loss: 0.5771\n",
      "Epoch: 2/20 Step: 55/390 ... Training loss: 0.5629\n",
      "Epoch: 2/20 Step: 56/390 ... Training loss: 0.5608\n",
      "Epoch: 2/20 Step: 57/390 ... Training loss: 0.5738\n",
      "Epoch: 2/20 Step: 58/390 ... Training loss: 0.5597\n",
      "Epoch: 2/20 Step: 59/390 ... Training loss: 0.5716\n",
      "Epoch: 2/20 Step: 60/390 ... Training loss: 0.5604\n",
      "Epoch: 2/20 Step: 61/390 ... Training loss: 0.5676\n",
      "Epoch: 2/20 Step: 62/390 ... Training loss: 0.5604\n",
      "Epoch: 2/20 Step: 63/390 ... Training loss: 0.5685\n",
      "Epoch: 2/20 Step: 64/390 ... Training loss: 0.5647\n",
      "Epoch: 2/20 Step: 65/390 ... Training loss: 0.5614\n",
      "Epoch: 2/20 Step: 66/390 ... Training loss: 0.5721\n",
      "Epoch: 2/20 Step: 67/390 ... Training loss: 0.5624\n",
      "Epoch: 2/20 Step: 68/390 ... Training loss: 0.5602\n",
      "Epoch: 2/20 Step: 69/390 ... Training loss: 0.5647\n",
      "Epoch: 2/20 Step: 70/390 ... Training loss: 0.5682\n",
      "Epoch: 2/20 Step: 71/390 ... Training loss: 0.5756\n",
      "Epoch: 2/20 Step: 72/390 ... Training loss: 0.5657\n",
      "Epoch: 2/20 Step: 73/390 ... Training loss: 0.5718\n",
      "Epoch: 2/20 Step: 74/390 ... Training loss: 0.5671\n",
      "Epoch: 2/20 Step: 75/390 ... Training loss: 0.5810\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "train_size = features.shape[0]\n",
    "batch_size = 128\n",
    "total_batches = train_size//batch_size\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for e in range(epochs):    \n",
    "    for ii in range(total_batches):\n",
    "        batch = get_batch(ii, batch_size)\n",
    "        imgs = batch[0].reshape((-1, 32, 32, 3))        \n",
    "        batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: imgs,\n",
    "                                                         targets_: imgs})\n",
    "        if ii % 100 == 0:\n",
    "            print(\"Epoch: {}/{} Step: {}/{} ...\".format(e+1, epochs, ii, total_batches),\n",
    "              \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "in_imgs = x_test[:10]\n",
    "reconstructed = sess.run(decoded, feed_dict={inputs_: in_imgs.reshape((10, 32, 32, 3))})\n",
    "\n",
    "for images, row in zip([in_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape(32, 32, 3))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Denoising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "inputs_ = tf.placeholder(tf.float32, (None, 32, 32, 3), name='inputs')\n",
    "targets_ = tf.placeholder(tf.float32, (None, 32, 32, 3), name='targets')\n",
    "\n",
    "### Encoder\n",
    "conv1 = tf.layers.conv2d(inputs_, 64, (3,3), padding='same', activation=tf.nn.relu)\n",
    "print(conv1.shape)\n",
    "# Now 32x32x64\n",
    "maxpool1 = tf.layers.max_pooling2d(conv1, (2,2), (2,2), padding='same')\n",
    "print(maxpool1.shape)\n",
    "# Now 16x16x64\n",
    "conv2 = tf.layers.conv2d(maxpool1, 128, (3,3), padding='same', activation=tf.nn.relu)\n",
    "print(conv2.shape)\n",
    "# Now 16x16x128\n",
    "maxpool2 = tf.layers.max_pooling2d(conv2, (2,2), (2,2), padding='same')\n",
    "print(maxpool2.shape)\n",
    "# Now 8x8x128\n",
    "conv3 = tf.layers.conv2d(maxpool2, 256, (3,3), padding='same', activation=tf.nn.relu)\n",
    "print(conv3.shape)\n",
    "# Now 8x8x256\n",
    "encoded = tf.layers.max_pooling2d(conv3, (2,2), (2,2), padding='same')\n",
    "print(encoded.shape)\n",
    "# Now 4x4x256\n",
    "\n",
    "### Decoder\n",
    "upsample1 = tf.image.resize_nearest_neighbor(encoded, (8,8))\n",
    "print(upsample1.shape)\n",
    "# Now 8x8x256\n",
    "conv4 = tf.layers.conv2d(upsample1, 128, (3,3), padding='same', activation=tf.nn.relu)\n",
    "print(conv4.shape)\n",
    "# Now 8x8x128\n",
    "upsample2 = tf.image.resize_nearest_neighbor(conv4, (16,16))\n",
    "print(upsample2.shape)\n",
    "# Now 16x16x128\n",
    "conv5 = tf.layers.conv2d(upsample2, 64, (3,3), padding='same', activation=tf.nn.relu)\n",
    "print(conv5.shape)\n",
    "# Now 16x16x64\n",
    "upsample3 = tf.image.resize_nearest_neighbor(conv5, (32,32))\n",
    "print(upsample3.shape)\n",
    "# Now 32x32x64\n",
    "logits = tf.layers.conv2d(upsample3, 3, (3,3), padding='same', activation=None)\n",
    "print(logits.shape)\n",
    "# Now 32x32x3\n",
    "\n",
    "# Pass logits through sigmoid to get reconstructed image\n",
    "decoded = tf.nn.sigmoid(logits, name='output')\n",
    "\n",
    "# Pass logits through sigmoid and calculate the cross-entropy loss\n",
    "loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_, logits=logits)\n",
    "\n",
    "# Get cost and define the optimizer\n",
    "cost = tf.reduce_mean(loss)\n",
    "opt = tf.train.AdamOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "batch_size = 128\n",
    "# Set's how much noise we're adding to the MNIST images\n",
    "noise_factor = 0.5\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for e in range(epochs):\n",
    "    for ii in range(total_batches):\n",
    "        batch = get_batch(batch_size)\n",
    "        # Get images from the batch\n",
    "        imgs = batch[0].reshape((-1, 32, 32, 3))\n",
    "        \n",
    "        # Add random noise to the input images\n",
    "        noisy_imgs = imgs + noise_factor * np.random.randn(*imgs.shape)\n",
    "        # Clip the images to be between 0 and 1\n",
    "        noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "        \n",
    "        # Noisy images as inputs, original images as targets\n",
    "        batch_cost, _ = sess.run([cost, opt], feed_dict={inputs_: noisy_imgs,\n",
    "                                                         targets_: imgs})\n",
    "\n",
    "        print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "              \"Training loss: {:.4f}\".format(batch_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking out the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))\n",
    "in_imgs = x_test[:10]\n",
    "noisy_imgs = in_imgs + noise_factor * np.random.randn(*in_imgs.shape)\n",
    "noisy_imgs = np.clip(noisy_imgs, 0., 1.)\n",
    "\n",
    "reconstructed = sess.run(decoded, feed_dict={inputs_: noisy_imgs.reshape((10, 32, 32, 3))})\n",
    "\n",
    "for images, row in zip([noisy_imgs, reconstructed], axes):\n",
    "    for img, ax in zip(images, row):\n",
    "        ax.imshow(img.reshape((32, 32, 3)))\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "\n",
    "fig.tight_layout(pad=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
